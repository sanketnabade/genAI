{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d8e8b0",
   "metadata": {},
   "source": [
    "*Text Processing:** **NLTK** and **SpaCy** are popular choices for tokenization, stemming, and other pre-processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630e7065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1ea1b",
   "metadata": {},
   "source": [
    "### Defining a Corpus\n",
    "\n",
    "A **corpus** is a collection of text. In Python, you can define a multi-line string using triple quotes. This will be our sample text for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767e5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''Hello.\n",
    "Welcome,Hello welcome to the Krish's NLP tutorials.\n",
    "Please do watch the entire course to become expert in NLP!\n",
    "Thank you so much!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82fc03",
   "metadata": {},
   "source": [
    "### Sentence Tokenization\n",
    "\n",
    "To split the corpus into sentences, we'll use the `sent_tokenize` function from NLTK. This function identifies sentence boundaries, typically indicated by punctuation like periods, exclamation marks, and question marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71595e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1240b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello.', \"Welcome,Hello welcome to the Krish's NLP tutorials.\", 'Please do watch the entire course to become expert in NLP!', 'Thank you so much!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cfcefb",
   "metadata": {},
   "source": [
    "This will output a list of sentences, where each sentence is a string.\n",
    "\n",
    "```\n",
    "['Hello.', \"Welcome,Hello welcome to the Krish's NLP tutorials.\", 'Please do watch the entire course to become expert in NLP!', 'Thank you so much!']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491881b2",
   "metadata": {},
   "source": [
    "### Word Tokenization\n",
    "\n",
    "To break down the text into individual words, we use the `word_tokenize` function. This function splits the text into words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ca280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '.', 'Welcome', ',', 'Hello', 'welcome', 'to', 'the', 'Krish', \"'s\", 'NLP', 'tutorials', '.', 'Please', 'do', 'watch', 'the', 'entire', 'course', 'to', 'become', 'expert', 'in', 'NLP', '!', 'Thank', 'you', 'so', 'much', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(corpus)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df25a15c",
   "metadata": {},
   "source": [
    "The output will be a list of words and punctuation.\n",
    "\n",
    "```\n",
    "['Hello', '.', 'Welcome', ',', 'Hello', 'welcome', 'to', 'the', 'Krish', \"'s\", 'NLP', 'tutorials', '.', 'Please', 'do', 'watch', 'the', 'entire', 'course', 'to', 'become', 'expert', 'in', 'NLP', '!', 'Thank', 'you', 'so', 'much', '!']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d7d31",
   "metadata": {},
   "source": [
    "### Exploring Other Tokenizers\n",
    "\n",
    "NLTK provides several other tokenizers that handle specific cases differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc89a8",
   "metadata": {},
   "source": [
    "#### **WordPunct Tokenizer**\n",
    "\n",
    "The `WordPunctTokenizer` is useful for scenarios where you need to separate punctuation from words. For example, it will split a word like \"welcome,\" into two tokens: \"welcome\" and \",\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc21ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '.', 'Welcome', ',', 'Hello', 'welcome', 'to', 'the', 'Krish', \"'\", 's', 'NLP', 'tutorials', '.', 'Please', 'do', 'watch', 'the', 'entire', 'course', 'to', 'become', 'expert', 'in', 'NLP', '!', 'Thank', 'you', 'so', 'much', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "print(wordpunct_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8065033a",
   "metadata": {},
   "source": [
    "The output highlights how punctuation is separated:\n",
    "\n",
    "```\n",
    "['Hello', '.', 'Welcome', ',', 'Hello', 'welcome', 'to', 'the', 'Krish', \"'\", 's', 'NLP', 'tutorials', '.', 'Please', 'do', 'watch', 'the', 'entire', 'course', 'to', 'become', 'expert', 'in', 'NLP', '!', 'Thank', 'you', 'so', 'much', '!']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4b248",
   "metadata": {},
   "source": [
    "#### **Treebank Word Tokenizer**\n",
    "\n",
    "The `TreebankWordTokenizer` is designed to handle contractions and other special cases in a more linguistically aware way. For example, it splits contractions like \"don't\" into \"do\" and \"n't\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb21ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello.', 'Welcome', ',', 'Hello', 'welcome', 'to', 'the', 'Krish', \"'s\", 'NLP', 'tutorials.', 'Please', 'do', 'watch', 'the', 'entire', 'course', 'to', 'become', 'expert', 'in', 'NLP', '!', 'Thank', 'you', 'so', 'much', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a4279",
   "metadata": {},
   "source": [
    "This tokenizer's output might be slightly different from `word_tokenize`, as it follows specific rules for handling punctuation.\n",
    "\n",
    "output:\n",
    "```bash\n",
    "['Hello.', 'Welcome', ',', 'Hello', 'welcome', 'to', 'the', 'Krish', \"'s\", 'NLP', 'tutorials.', 'Please', 'do', 'watch', 'the', 'entire', 'course', 'to', 'become', 'expert', 'in', 'NLP', '!', 'Thank', 'you', 'so', 'much', '!']\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
